# DD4RL

## 采用分离的观察和与RL相同的梯度分配
这样会导致蒸馏数据集中的奖励以及下一步奖励梯度取消，无法学习。或者说只能学习观察和动作以匹配梯度。

## 采用五元组式的蒸馏数据和与RL不一致的梯度分配
虽然能够学到五元组中的每一种数据（`obs, action, discount, reward, next_obs`），但是由于梯度与RL中的不一致，并且对五元组没有任何的约束，导致最终学到的蒸馏数据几乎是空白的，如下，并且在此基础上的智能体训练无法收敛：

![](docs/test_1.png)


## 采用轨迹式的蒸馏数据和与RL相同的梯度分配
代码实现会比较复杂，但是当前观察和下一步观察都能够学习到（但是轨迹最后的观察无法学习），另外对于奖励函数不再通过蒸馏获取，而是额外的奖励预测网络进行学习。


## 对比
保留回报较高的轨迹进行二次学习，观察是否有加速现象

按比例保留不同回报区间的轨迹进行二次学习，观察是否有加速现象