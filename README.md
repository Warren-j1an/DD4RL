# DD4RL

## 动机
利用数据蒸馏将强化学习的经验回放池蒸馏成小数据集，之后利用蒸馏数据集可以快速地从零训练智能体，使其能够达到和源智能体相似的性能。

## 存在的问题
强化学习经验回放池中的数据并非用于分类的数据集，即没有硬标签。而目前已有的数据蒸馏方法都是基于分类任务进行蒸馏。

最直观的思路是采用五元组中的奖励作为硬标签蒸馏数据。这会将所有相同奖励的五元组分为一类，不过相同奖励下的状五元组的Q值并不相同。强化学习算法无论如何都绕不开对Q值的学习，如果蒸馏数据集将所有不同Q值的五元组“融合”成一个五元组，将其中的状态动作对输入Q网络，得到的是哪一个的Q值呢？这势必导致Q网络的不收敛。实验结果也验证了这一点。

另外一种对经验回放池中数据分类的方法则是基于回报，把同一Q值的五元组分为同一类，将Q值作为硬标签。这里存在着另外一个问题，即Q值在智能体学习的过程中是不断变化的。下面就是主要围绕这个问题进行讨论。