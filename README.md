# DD4RL

## 分离式的观察
使用分离式的当前观察`obs`和下一步观察`next_obs`。
但是为了保证这两种数据都能够被学习，修改了强化学习`critic`的梯度传播。
一般来说通过`target`网络获取下一步观察的目标值是无梯度的，但是为了增加对`next_obs`的梯度，加上了这一部分的梯度。

## 固定折扣因子和奖励
将单步奖励分为1000等份。（单步奖励包括：`action_repeat * nstep`）。对真实数据的奖励和蒸馏数据的奖励一一对应，并通过DC进行数据蒸馏。

## 梯度选择
最开始只选择`critic`网络进行梯度回传，但是这会导致`obs`和`next_obs`梯度量级为`1e-8`，因为最多只有`1e6`的学习外加小于`0.01`的学习率，最终能够更改的值在`1e-4`左右，对数据更新过小，几乎没有更新。

但是加上`actor`网络后，梯度量级可以达到`1e-5`左右，可以进行有效学习。